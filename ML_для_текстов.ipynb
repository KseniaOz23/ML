{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from tqdm import notebook\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_news= pd.read_csv('/datasets/news_data.csv')\n",
        "df_news= df_news.sample(400, random_state=42).reset_index(drop=True)\n",
        "\n",
        "tokenizer = transformers.BertTokenizer(\n",
        "    vocab_file='/datasets/ds_bert/vocab.txt')\n",
        "\n",
        "max_len = 512\n",
        "\n",
        "tokenized = df_news['text'].apply(\n",
        "    lambda x: np.array(\n",
        "        tokenizer.encode(\n",
        "            x,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            truncation=True,\n",
        "            padding='max_length'\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "padded = np.array(tokenized.values.tolist())\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "\n",
        "config = transformers.BertConfig.from_json_file(\n",
        "    '/datasets/ds_bert/bert_config.json')\n",
        "model = transformers.BertModel.from_pretrained(\n",
        "    '/datasets/ds_bert/rubert_model.bin', config=config)\n",
        "\n",
        "batch_size = 100\n",
        "embeddings = []\n",
        "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
        "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)])\n",
        "        attention_mask_batch = torch.LongTensor(\n",
        "            attention_mask[batch_size*i:batch_size*(i+1)]\n",
        "          )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
        "\n",
        "        embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
        "\n",
        "features = np.concatenate(embeddings)\n",
        "target = df_news['rubric']\n",
        "\n",
        "train_features, test_features, train_target, test_target = train_test_split(\n",
        "    features, target, test_size=200, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(train_features, train_target)\n",
        "\n",
        "print(model.score(test_features, test_target))"
      ],
      "metadata": {
        "id": "fEf-mt23tA4S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}